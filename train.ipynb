{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "one thousand two hundred thirty four\n"
     ]
    }
   ],
   "source": [
    "# Python program to print a given number in\n",
    "# words. The program handles numbers\n",
    "# from 0 to 9999\n",
    "  \n",
    "  \n",
    "def convert_to_words(num):\n",
    "    result = \"\"\n",
    "  \n",
    "    # Get number of digits\n",
    "    # in given number\n",
    "    l = len(num)\n",
    "  \n",
    "    # Base cases\n",
    "    if (l == 0):\n",
    "        return \"empty string\"\n",
    "  \n",
    "    if (l > 4):\n",
    "        return \"Length more than 4 is not supported\"\n",
    "  \n",
    "    # The first string is not used,\n",
    "    # it is to make array indexing simple\n",
    "    single_digits = [\"zero\", \"one\", \"two\", \"three\",\n",
    "                     \"four\", \"five\", \"six\", \"seven\",\n",
    "                     \"eight\", \"nine\"]\n",
    "  \n",
    "    # The first string is not used,\n",
    "    # it is to make array indexing simple\n",
    "    two_digits = [\"\", \"ten\", \"eleven\", \"twelve\",\n",
    "                  \"thirteen\", \"fourteen\", \"fifteen\",\n",
    "                  \"sixteen\", \"seventeen\", \"eighteen\",\n",
    "                  \"nineteen\"]\n",
    "  \n",
    "    # The first two string are not used,\n",
    "    # they are to make array indexing simple\n",
    "    tens_multiple = [\"\", \"\", \"twenty\", \"thirty\", \"forty\",\n",
    "                     \"fifty\", \"sixty\", \"seventy\", \"eighty\",\n",
    "                     \"ninety\"]\n",
    "  \n",
    "    tens_power = [\"hundred\", \"thousand\"]\n",
    "  \n",
    "    # For single digit number\n",
    "    if (l == 1):\n",
    "        return (single_digits[ord(num[0]) - 48])\n",
    "  \n",
    "    # Iterate while num is not '\\0'\n",
    "    x = 0\n",
    "    while (x < len(num)):\n",
    "  \n",
    "        # Code path for first 2 digits\n",
    "        if (l >= 3):\n",
    "            if (ord(num[x]) - 48 != 0):\n",
    "               result += single_digits[ord(num[x]) - 48] + \" \"\n",
    "               result += tens_power[l - 3] + \" \"\n",
    "                # here len can be 3 or 4\n",
    "  \n",
    "            l -= 1\n",
    "  \n",
    "        # Code path for last 2 digits\n",
    "        else:\n",
    "  \n",
    "            # Need to explicitly handle\n",
    "            # 10-19. Sum of the two digits\n",
    "            # is used as index of \"two_digits\"\n",
    "            # array of strings\n",
    "            if (ord(num[x]) - 48 == 1):\n",
    "                sum = (ord(num[x]) - 48 +\n",
    "                       ord(num[x+1]) - 48)\n",
    "                result += two_digits[sum]\n",
    "                return result\n",
    "  \n",
    "            # Need to explicitly handle 20\n",
    "            elif (ord(num[x]) - 48 == 2 and\n",
    "                  ord(num[x + 1]) - 48 == 0):\n",
    "                result += \"twenty\"\n",
    "                return result\n",
    "  \n",
    "            # Rest of the two digit\n",
    "            # numbers i.e., 21 to 99\n",
    "            else:\n",
    "                i = ord(num[x]) - 48\n",
    "                if(i > 0):\n",
    "                    result += tens_multiple[i] + \" \"\n",
    "                x += 1\n",
    "                if(ord(num[x]) - 48 != 0):\n",
    "                    result += single_digits[ord(num[x]) - 48]\n",
    "        x += 1\n",
    "    return result\n",
    "  \n",
    "# This code is contributed\n",
    "# by Mithun Kumar\n",
    "\n",
    "print(convert_to_words(\"1234\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (1.13.0)\n",
      "Requirement already satisfied: typing-extensions in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from torch) (4.4.0)\n"
     ]
    }
   ],
   "source": [
    "!pip3 install torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Network(\n",
      "  (fc1): Linear(in_features=180, out_features=120, bias=True)\n",
      "  (fc2): Linear(in_features=120, out_features=100, bias=True)\n",
      "  (fc3): Linear(in_features=100, out_features=10000, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch import nn\n",
    "\n",
    "# define the network class\n",
    "class Network(nn.Module):\n",
    "    def __init__(self):\n",
    "        # call constructor from superclass\n",
    "        super().__init__()\n",
    "        \n",
    "        # define network layers\n",
    "        self.fc1 = nn.Linear(6 * 30, 120)\n",
    "        self.fc2 = nn.Linear(120, 100)\n",
    "        self.fc3 = nn.Linear(100, 10000)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # define forward pass\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = torch.sigmoid(self.fc3(x))\n",
    "        return x\n",
    "\n",
    "# instantiate the model\n",
    "model = Network()\n",
    "\n",
    "# print model architecture\n",
    "print(model)\n",
    "\n",
    "# fully connected network from https://towardsdatascience.com/three-ways-to-build-a-neural-network-in-pytorch-8cea49f9a61a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/gz/pp4ldsnj0tjbngm2gygkp8qc0000gn/T/ipykernel_41421/3808141098.py:3: UserWarning: nn.init.xavier_uniform is now deprecated in favor of nn.init.xavier_uniform_.\n",
      "  torch.nn.init.xavier_uniform(m.weight)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Network(\n",
       "  (fc1): Linear(in_features=180, out_features=120, bias=True)\n",
       "  (fc2): Linear(in_features=120, out_features=100, bias=True)\n",
       "  (fc3): Linear(in_features=100, out_features=10000, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def init_weights(m):\n",
    "    if isinstance(m, nn.Linear):\n",
    "        torch.nn.init.xavier_uniform(m.weight)\n",
    "        m.bias.data.fill_(0.01)\n",
    "\n",
    "model.apply(init_weights)\n",
    "\n",
    "# from https://stackoverflow.com/questions/49433936/how-do-i-initialize-weights-in-pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "# Optimizers specified in the torch.optim package\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.001, momentum=0.9)\n",
    "\n",
    "# from https://pytorch.org/tutorials/beginner/introyt/trainingyt.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9 9917\n",
      "9 9465\n",
      "8 8465\n",
      "5 5231\n",
      "6 6641\n",
      "9 9419\n",
      "4 4054\n",
      "5 5550\n",
      "5 5130\n",
      "6 6396\n",
      "5 5235\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "def getTrainingData(n):\n",
    "    for _ in range(n+1):\n",
    "        n = random.randint(0, 9999)\n",
    "        num = convert_to_words(str(n))\n",
    "        # print(num)\n",
    "        parts = num.split(\" \")\n",
    "        categories = [\"zero\", \"one\", \"two\", \"three\",\n",
    "                     \"four\", \"five\", \"six\", \"seven\",\n",
    "                     \"eight\", \"nine\", \"ten\", \"eleven\", \"twelve\",\n",
    "                     \"thirteen\", \"fourteen\", \"fifteen\",\n",
    "                     \"sixteen\", \"seventeen\", \"eighteen\",\n",
    "                     \"nineteen\", \"twenty\", \"thirty\", \"forty\",\n",
    "                     \"fifty\", \"sixty\", \"seventy\", \"eighty\",\n",
    "                     \"ninety\", \"hundred\", \"thousand\"]\n",
    "        values = []\n",
    "        i = 0\n",
    "        for part in parts:\n",
    "            if part in categories and i < 6:\n",
    "                values += [categories.index(part) + i * 30]\n",
    "                i += 1\n",
    "        x = torch.zeros(1, 180)\n",
    "        ixs = torch.tensor(values)\n",
    "        yield (x.index_add(1, ixs, torch.ones(1, len(values))), F.one_hot(torch.tensor([n]), 10000).double())\n",
    "\n",
    "for d in getTrainingData(10):\n",
    "    print(d[0].argmax().tolist(), d[1].argmax().tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_one_epoch(epoch_index):\n",
    "    running_loss = 0.\n",
    "    last_loss = 0.\n",
    "\n",
    "    datas = list(getTrainingData(9999))\n",
    "    for i, data in enumerate(datas):\n",
    "        # Every data instance is an input + label pair\n",
    "        inputs, labels = data\n",
    "\n",
    "        # Zero your gradients for every batch!\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Make predictions for this batch\n",
    "        outputs = model(inputs)\n",
    "\n",
    "        # Compute the loss and its gradients\n",
    "        loss = loss_fn(outputs, labels)\n",
    "        loss.backward()\n",
    "\n",
    "        # Adjust learning weights\n",
    "        optimizer.step()\n",
    "\n",
    "        # Gather data and report\n",
    "        running_loss += loss.item()\n",
    "        if i % 1000 == 999:\n",
    "            last_loss = running_loss / 1000 # loss per batch\n",
    "            print('  batch {} loss: {}'.format(i + 1, last_loss))\n",
    "            tb_x = epoch_index * len(datas) + i + 1\n",
    "            running_loss = 0.\n",
    "\n",
    "    return last_loss\n",
    "\n",
    "    # from https://pytorch.org/tutorials/beginner/introyt/trainingyt.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH 1:\n",
      "  batch 1000 loss: 9.210403964042664\n",
      "  batch 2000 loss: 9.210341051101684\n",
      "  batch 3000 loss: 9.210375742912293\n",
      "  batch 4000 loss: 9.210262008666993\n",
      "  batch 5000 loss: 9.210280695915221\n",
      "  batch 6000 loss: 9.210167917251587\n",
      "  batch 7000 loss: 9.210229321479797\n",
      "  batch 8000 loss: 9.210170796394348\n",
      "  batch 9000 loss: 9.21015788078308\n",
      "  batch 10000 loss: 9.210197590827942\n",
      "LOSS train9.210197590827942\n",
      "EPOCH 2:\n",
      "  batch 1000 loss: 9.210007716178895\n",
      "  batch 2000 loss: 9.21013210582733\n",
      "  batch 3000 loss: 9.210206009864807\n",
      "  batch 4000 loss: 9.209819379806518\n",
      "  batch 5000 loss: 9.209968841552735\n",
      "  batch 6000 loss: 9.210041770935058\n",
      "  batch 7000 loss: 9.20993950366974\n",
      "  batch 8000 loss: 9.209810066223145\n",
      "  batch 9000 loss: 9.209686321258545\n",
      "  batch 10000 loss: 9.209977289199829\n",
      "LOSS train9.209977289199829\n",
      "EPOCH 3:\n",
      "  batch 1000 loss: 9.209915906906128\n",
      "  batch 2000 loss: 9.209839703559876\n",
      "  batch 3000 loss: 9.20986626625061\n",
      "  batch 4000 loss: 9.20989029598236\n",
      "  batch 5000 loss: 9.209641083717345\n",
      "  batch 6000 loss: 9.209830615997314\n",
      "  batch 7000 loss: 9.209667155265809\n",
      "  batch 8000 loss: 9.209397552490234\n",
      "  batch 9000 loss: 9.209636601448059\n",
      "  batch 10000 loss: 9.209222734451293\n",
      "LOSS train9.209222734451293\n",
      "EPOCH 4:\n",
      "  batch 1000 loss: 9.20942900943756\n",
      "  batch 2000 loss: 9.209363522529602\n",
      "  batch 3000 loss: 9.209653210639953\n",
      "  batch 4000 loss: 9.20925577545166\n",
      "  batch 5000 loss: 9.209628950119018\n",
      "  batch 6000 loss: 9.20929657459259\n",
      "  batch 7000 loss: 9.209446821212769\n",
      "  batch 8000 loss: 9.209389630317688\n",
      "  batch 9000 loss: 9.209061408996583\n",
      "  batch 10000 loss: 9.209438398361206\n",
      "LOSS train9.209438398361206\n",
      "EPOCH 5:\n",
      "  batch 1000 loss: 9.20927149963379\n",
      "  batch 2000 loss: 9.208827311515808\n",
      "  batch 3000 loss: 9.208737363815308\n",
      "  batch 4000 loss: 9.208802574157716\n",
      "  batch 5000 loss: 9.208974975585937\n",
      "  batch 6000 loss: 9.209303244590759\n",
      "  batch 7000 loss: 9.208833463668823\n",
      "  batch 8000 loss: 9.208744897842408\n",
      "  batch 9000 loss: 9.208900444984437\n",
      "  batch 10000 loss: 9.208722821235657\n",
      "LOSS train9.208722821235657\n",
      "EPOCH 6:\n",
      "  batch 1000 loss: 9.208720241546631\n",
      "  batch 2000 loss: 9.208785251617432\n",
      "  batch 3000 loss: 9.20852927684784\n",
      "  batch 4000 loss: 9.208402970314026\n",
      "  batch 5000 loss: 9.20845257949829\n",
      "  batch 6000 loss: 9.208658713340759\n",
      "  batch 7000 loss: 9.208138255119323\n",
      "  batch 8000 loss: 9.208305342674254\n",
      "  batch 9000 loss: 9.208108684539795\n",
      "  batch 10000 loss: 9.207887143135071\n",
      "LOSS train9.207887143135071\n",
      "EPOCH 7:\n",
      "  batch 1000 loss: 9.208123907089233\n",
      "  batch 2000 loss: 9.207925219535827\n",
      "  batch 3000 loss: 9.207756706237793\n",
      "  batch 4000 loss: 9.207975068092345\n",
      "  batch 5000 loss: 9.207679152488708\n",
      "  batch 6000 loss: 9.207696449279785\n",
      "  batch 7000 loss: 9.207883060455321\n",
      "  batch 8000 loss: 9.208117051124573\n",
      "  batch 9000 loss: 9.207256097793579\n",
      "  batch 10000 loss: 9.207497480392457\n",
      "LOSS train9.207497480392457\n",
      "EPOCH 8:\n",
      "  batch 1000 loss: 9.20770168018341\n",
      "  batch 2000 loss: 9.207303997993469\n",
      "  batch 3000 loss: 9.207436729431153\n",
      "  batch 4000 loss: 9.20751037120819\n",
      "  batch 5000 loss: 9.207785724639892\n",
      "  batch 6000 loss: 9.206903037071228\n",
      "  batch 7000 loss: 9.207108598709107\n",
      "  batch 8000 loss: 9.206584882736205\n",
      "  batch 9000 loss: 9.206304533958434\n",
      "  batch 10000 loss: 9.206295904159546\n",
      "LOSS train9.206295904159546\n",
      "EPOCH 9:\n",
      "  batch 1000 loss: 9.206516122817993\n",
      "  batch 2000 loss: 9.206564989089966\n",
      "  batch 3000 loss: 9.206702793121337\n",
      "  batch 4000 loss: 9.206288996696472\n",
      "  batch 5000 loss: 9.206113158226014\n",
      "  batch 6000 loss: 9.205945609092712\n",
      "  batch 7000 loss: 9.205652642250062\n",
      "  batch 8000 loss: 9.205631220817565\n",
      "  batch 9000 loss: 9.205897822380066\n",
      "  batch 10000 loss: 9.205921521186829\n",
      "LOSS train9.205921521186829\n",
      "EPOCH 10:\n",
      "  batch 1000 loss: 9.204781623840333\n",
      "  batch 2000 loss: 9.205263848304748\n",
      "  batch 3000 loss: 9.204519634246827\n",
      "  batch 4000 loss: 9.204894241333008\n",
      "  batch 5000 loss: 9.205310685157777\n",
      "  batch 6000 loss: 9.204627716064453\n",
      "  batch 7000 loss: 9.204623574256898\n",
      "  batch 8000 loss: 9.20417471408844\n",
      "  batch 9000 loss: 9.204428208351136\n",
      "  batch 10000 loss: 9.203648370742798\n",
      "LOSS train9.203648370742798\n",
      "EPOCH 11:\n",
      "  batch 1000 loss: 9.203511944770813\n",
      "  batch 2000 loss: 9.204763411521911\n",
      "  batch 3000 loss: 9.203804651260375\n",
      "  batch 4000 loss: 9.202443675994873\n",
      "  batch 5000 loss: 9.202406192779542\n",
      "  batch 6000 loss: 9.202475534439087\n",
      "  batch 7000 loss: 9.20223736858368\n",
      "  batch 8000 loss: 9.203005291938782\n",
      "  batch 9000 loss: 9.202057282447814\n",
      "  batch 10000 loss: 9.20226108455658\n",
      "LOSS train9.20226108455658\n",
      "EPOCH 12:\n",
      "  batch 1000 loss: 9.201753308296203\n",
      "  batch 2000 loss: 9.201782471656799\n",
      "  batch 3000 loss: 9.20218226146698\n",
      "  batch 4000 loss: 9.201567900657654\n",
      "  batch 5000 loss: 9.199955030441284\n",
      "  batch 6000 loss: 9.198820838928222\n",
      "  batch 7000 loss: 9.200656287193299\n",
      "  batch 8000 loss: 9.199238493919372\n",
      "  batch 9000 loss: 9.199375902175904\n",
      "  batch 10000 loss: 9.199236382484436\n",
      "LOSS train9.199236382484436\n",
      "EPOCH 13:\n",
      "  batch 1000 loss: 9.197793586730956\n",
      "  batch 2000 loss: 9.197391480445862\n",
      "  batch 3000 loss: 9.197590907096863\n",
      "  batch 4000 loss: 9.19704344367981\n",
      "  batch 5000 loss: 9.198184688568116\n",
      "  batch 6000 loss: 9.196766465187073\n",
      "  batch 7000 loss: 9.195911569595337\n",
      "  batch 8000 loss: 9.195055727005006\n",
      "  batch 9000 loss: 9.195254042625427\n",
      "  batch 10000 loss: 9.193730319023132\n",
      "LOSS train9.193730319023132\n",
      "EPOCH 14:\n",
      "  batch 1000 loss: 9.195640569686889\n",
      "  batch 2000 loss: 9.19394954109192\n",
      "  batch 3000 loss: 9.193164156913758\n",
      "  batch 4000 loss: 9.193816071510314\n",
      "  batch 5000 loss: 9.193563400268555\n",
      "  batch 6000 loss: 9.191662818908691\n",
      "  batch 7000 loss: 9.189008687973022\n",
      "  batch 8000 loss: 9.190694762229919\n",
      "  batch 9000 loss: 9.189979858398438\n",
      "  batch 10000 loss: 9.188541687965394\n",
      "LOSS train9.188541687965394\n",
      "EPOCH 15:\n",
      "  batch 1000 loss: 9.18852632522583\n",
      "  batch 2000 loss: 9.189593527793884\n",
      "  batch 3000 loss: 9.187160326004028\n",
      "  batch 4000 loss: 9.186385466575622\n",
      "  batch 5000 loss: 9.18475809764862\n",
      "  batch 6000 loss: 9.184266041755675\n",
      "  batch 7000 loss: 9.18588994884491\n",
      "  batch 8000 loss: 9.182684960365295\n",
      "  batch 9000 loss: 9.183388569831848\n",
      "  batch 10000 loss: 9.180321907997131\n",
      "LOSS train9.180321907997131\n",
      "EPOCH 16:\n",
      "  batch 1000 loss: 9.177498439788819\n",
      "  batch 2000 loss: 9.177313985824584\n",
      "  batch 3000 loss: 9.175391073226928\n",
      "  batch 4000 loss: 9.172689973831178\n",
      "  batch 5000 loss: 9.175951897621156\n",
      "  batch 6000 loss: 9.173501238822936\n",
      "  batch 7000 loss: 9.172311980247498\n",
      "  batch 8000 loss: 9.17300800228119\n",
      "  batch 9000 loss: 9.172080441474915\n",
      "  batch 10000 loss: 9.17314657688141\n",
      "LOSS train9.17314657688141\n",
      "EPOCH 17:\n",
      "  batch 1000 loss: 9.166389525413512\n",
      "  batch 2000 loss: 9.172697823524475\n",
      "  batch 3000 loss: 9.162694422721863\n",
      "  batch 4000 loss: 9.16131631565094\n",
      "  batch 5000 loss: 9.160144561767579\n",
      "  batch 6000 loss: 9.16308828639984\n",
      "  batch 7000 loss: 9.160620109558106\n",
      "  batch 8000 loss: 9.165642412185669\n",
      "  batch 9000 loss: 9.159019536018372\n",
      "  batch 10000 loss: 9.14807121181488\n",
      "LOSS train9.14807121181488\n",
      "EPOCH 18:\n",
      "  batch 1000 loss: 9.150602373123169\n",
      "  batch 2000 loss: 9.155517939567567\n",
      "  batch 3000 loss: 9.149358687400818\n",
      "  batch 4000 loss: 9.151941393852233\n",
      "  batch 5000 loss: 9.147000167846679\n",
      "  batch 6000 loss: 9.143635272026062\n",
      "  batch 7000 loss: 9.150842127799988\n",
      "  batch 8000 loss: 9.143947532653808\n",
      "  batch 9000 loss: 9.148400151252746\n",
      "  batch 10000 loss: 9.137789353370666\n",
      "LOSS train9.137789353370666\n",
      "EPOCH 19:\n",
      "  batch 1000 loss: 9.141331608772278\n",
      "  batch 2000 loss: 9.128718536376953\n",
      "  batch 3000 loss: 9.135279931068421\n",
      "  batch 4000 loss: 9.134781695365906\n",
      "  batch 5000 loss: 9.123726022720337\n",
      "  batch 6000 loss: 9.127521682739259\n",
      "  batch 7000 loss: 9.136487992286682\n",
      "  batch 8000 loss: 9.127767492294312\n",
      "  batch 9000 loss: 9.113660774230958\n",
      "  batch 10000 loss: 9.122910362243653\n",
      "LOSS train9.122910362243653\n",
      "EPOCH 20:\n",
      "  batch 1000 loss: 9.120013845443726\n",
      "  batch 2000 loss: 9.114992253303528\n",
      "  batch 3000 loss: 9.123329827308655\n",
      "  batch 4000 loss: 9.11192949295044\n",
      "  batch 5000 loss: 9.11231890296936\n",
      "  batch 6000 loss: 9.111422818183899\n",
      "  batch 7000 loss: 9.112563254356385\n",
      "  batch 8000 loss: 9.101462151527405\n",
      "  batch 9000 loss: 9.109420083999634\n",
      "  batch 10000 loss: 9.09841343307495\n",
      "LOSS train9.09841343307495\n",
      "EPOCH 21:\n",
      "  batch 1000 loss: 9.101910922050475\n",
      "  batch 2000 loss: 9.09482795906067\n",
      "  batch 3000 loss: 9.087898436546325\n",
      "  batch 4000 loss: 9.09419253063202\n",
      "  batch 5000 loss: 9.083982697486878\n",
      "  batch 6000 loss: 9.081111485481262\n",
      "  batch 7000 loss: 9.071113931655884\n",
      "  batch 8000 loss: 9.079278414726257\n",
      "  batch 9000 loss: 9.068182657241822\n",
      "  batch 10000 loss: 9.071690745353699\n",
      "LOSS train9.071690745353699\n",
      "EPOCH 22:\n",
      "  batch 1000 loss: 9.07691603755951\n",
      "  batch 2000 loss: 9.061399662971496\n",
      "  batch 3000 loss: 9.057480989456177\n",
      "  batch 4000 loss: 9.054763360977173\n",
      "  batch 5000 loss: 9.055755277633667\n",
      "  batch 6000 loss: 9.040900869369507\n",
      "  batch 7000 loss: 9.042165432929993\n",
      "  batch 8000 loss: 9.039580980300903\n",
      "  batch 9000 loss: 9.036756237983704\n",
      "  batch 10000 loss: 9.035183184623719\n",
      "LOSS train9.035183184623719\n",
      "EPOCH 23:\n",
      "  batch 1000 loss: 9.024135038375855\n",
      "  batch 2000 loss: 9.02025942325592\n",
      "  batch 3000 loss: 9.013689732551574\n",
      "  batch 4000 loss: 9.005157927513123\n",
      "  batch 5000 loss: 9.003177712440491\n",
      "  batch 6000 loss: 8.999598887443543\n",
      "  batch 7000 loss: 8.991981271743775\n",
      "  batch 8000 loss: 8.97565136051178\n",
      "  batch 9000 loss: 8.980076581001281\n",
      "  batch 10000 loss: 8.976643682479859\n",
      "LOSS train8.976643682479859\n",
      "EPOCH 24:\n",
      "  batch 1000 loss: 8.968924311637878\n",
      "  batch 2000 loss: 8.961063796043396\n",
      "  batch 3000 loss: 8.96239402961731\n",
      "  batch 4000 loss: 8.94808779335022\n",
      "  batch 5000 loss: 8.938385137557983\n",
      "  batch 6000 loss: 8.933650203704834\n",
      "  batch 7000 loss: 8.928361952781676\n",
      "  batch 8000 loss: 8.923167839050294\n",
      "  batch 9000 loss: 8.909881066322326\n",
      "  batch 10000 loss: 8.897623463630676\n",
      "LOSS train8.897623463630676\n",
      "EPOCH 25:\n",
      "  batch 1000 loss: 8.904239385604859\n",
      "  batch 2000 loss: 8.891283393859863\n",
      "  batch 3000 loss: 8.892097978591918\n",
      "  batch 4000 loss: 8.870957869529724\n",
      "  batch 5000 loss: 8.86829247379303\n",
      "  batch 6000 loss: 8.876128076553345\n",
      "  batch 7000 loss: 8.858603966712952\n",
      "  batch 8000 loss: 8.842702797889709\n",
      "  batch 9000 loss: 8.837827962875366\n",
      "  batch 10000 loss: 8.831822171211243\n",
      "LOSS train8.831822171211243\n",
      "EPOCH 26:\n",
      "  batch 1000 loss: 8.834513155937195\n",
      "  batch 2000 loss: 8.814834046363831\n",
      "  batch 3000 loss: 8.81671252155304\n",
      "  batch 4000 loss: 8.800588537216187\n",
      "  batch 5000 loss: 8.799530094146728\n",
      "  batch 6000 loss: 8.798075988769531\n",
      "  batch 7000 loss: 8.784236950874329\n",
      "  batch 8000 loss: 8.768070881843567\n",
      "  batch 9000 loss: 8.768198095321655\n",
      "  batch 10000 loss: 8.756829840660096\n",
      "LOSS train8.756829840660096\n",
      "EPOCH 27:\n",
      "  batch 1000 loss: 8.74762755393982\n",
      "  batch 2000 loss: 8.74160517501831\n",
      "  batch 3000 loss: 8.740028473854064\n",
      "  batch 4000 loss: 8.732909284591674\n",
      "  batch 5000 loss: 8.715892147064208\n",
      "  batch 6000 loss: 8.71008410167694\n",
      "  batch 7000 loss: 8.71351794719696\n",
      "  batch 8000 loss: 8.70251188659668\n",
      "  batch 9000 loss: 8.703501519203186\n",
      "  batch 10000 loss: 8.688982191085815\n",
      "LOSS train8.688982191085815\n",
      "EPOCH 28:\n",
      "  batch 1000 loss: 8.684726088523865\n",
      "  batch 2000 loss: 8.675243238449097\n",
      "  batch 3000 loss: 8.682146532058717\n",
      "  batch 4000 loss: 8.657320623397828\n",
      "  batch 5000 loss: 8.649557913780212\n",
      "  batch 6000 loss: 8.65314241695404\n",
      "  batch 7000 loss: 8.639560750961303\n",
      "  batch 8000 loss: 8.642259182929992\n",
      "  batch 9000 loss: 8.637238931655883\n",
      "  batch 10000 loss: 8.629490132331847\n",
      "LOSS train8.629490132331847\n",
      "EPOCH 29:\n",
      "  batch 1000 loss: 8.62122533607483\n",
      "  batch 2000 loss: 8.60898136806488\n",
      "  batch 3000 loss: 8.619148034095764\n",
      "  batch 4000 loss: 8.608890979766846\n",
      "  batch 5000 loss: 8.602558208465576\n",
      "  batch 6000 loss: 8.593366353034973\n",
      "  batch 7000 loss: 8.595985008239746\n",
      "  batch 8000 loss: 8.586393634796142\n",
      "  batch 9000 loss: 8.572249987602234\n",
      "  batch 10000 loss: 8.589416655540466\n",
      "LOSS train8.589416655540466\n",
      "EPOCH 30:\n",
      "  batch 1000 loss: 8.576652265548706\n",
      "  batch 2000 loss: 8.577442947387695\n",
      "  batch 3000 loss: 8.567802292823792\n",
      "  batch 4000 loss: 8.569832348823548\n",
      "  batch 5000 loss: 8.5570832529068\n",
      "  batch 6000 loss: 8.549713265419006\n",
      "  batch 7000 loss: 8.549625170707703\n",
      "  batch 8000 loss: 8.54592765903473\n",
      "  batch 9000 loss: 8.547888003349303\n",
      "  batch 10000 loss: 8.538365479469299\n",
      "LOSS train8.538365479469299\n",
      "EPOCH 31:\n",
      "  batch 1000 loss: 8.532253067016601\n",
      "  batch 2000 loss: 8.521751224517823\n",
      "  batch 3000 loss: 8.516912722587586\n",
      "  batch 4000 loss: 8.518990036964416\n",
      "  batch 5000 loss: 8.518165258407592\n",
      "  batch 6000 loss: 8.521723017692565\n",
      "  batch 7000 loss: 8.516413805961609\n",
      "  batch 8000 loss: 8.508386373519897\n",
      "  batch 9000 loss: 8.50174391078949\n",
      "  batch 10000 loss: 8.502743227958678\n",
      "LOSS train8.502743227958678\n",
      "EPOCH 32:\n",
      "  batch 1000 loss: 8.504548927307129\n",
      "  batch 2000 loss: 8.49601209640503\n",
      "  batch 3000 loss: 8.498169679641723\n",
      "  batch 4000 loss: 8.492460227012634\n",
      "  batch 5000 loss: 8.484741752624512\n",
      "  batch 6000 loss: 8.482055756568908\n",
      "  batch 7000 loss: 8.479734365463257\n",
      "  batch 8000 loss: 8.477593627929688\n",
      "  batch 9000 loss: 8.47226445198059\n",
      "  batch 10000 loss: 8.468232283592224\n",
      "LOSS train8.468232283592224\n",
      "EPOCH 33:\n",
      "  batch 1000 loss: 8.466755354881286\n",
      "  batch 2000 loss: 8.4695854139328\n",
      "  batch 3000 loss: 8.465366531372071\n",
      "  batch 4000 loss: 8.459864066123963\n",
      "  batch 5000 loss: 8.45119987487793\n",
      "  batch 6000 loss: 8.452808655738831\n",
      "  batch 7000 loss: 8.447798329353333\n",
      "  batch 8000 loss: 8.451722276687622\n",
      "  batch 9000 loss: 8.451716533660889\n",
      "  batch 10000 loss: 8.450962394714356\n",
      "LOSS train8.450962394714356\n",
      "EPOCH 34:\n",
      "  batch 1000 loss: 8.45033536529541\n",
      "  batch 2000 loss: 8.44185244178772\n",
      "  batch 3000 loss: 8.442760518074035\n",
      "  batch 4000 loss: 8.435321012496948\n",
      "  batch 5000 loss: 8.438715720176697\n",
      "  batch 6000 loss: 8.436351265907287\n",
      "  batch 7000 loss: 8.42972968006134\n",
      "  batch 8000 loss: 8.42890738773346\n",
      "  batch 9000 loss: 8.428051630973815\n",
      "  batch 10000 loss: 8.424327357292174\n",
      "LOSS train8.424327357292174\n",
      "EPOCH 35:\n",
      "  batch 1000 loss: 8.429981490135193\n",
      "  batch 2000 loss: 8.42566452407837\n",
      "  batch 3000 loss: 8.42270414352417\n",
      "  batch 4000 loss: 8.412192722320556\n",
      "  batch 5000 loss: 8.41832067489624\n",
      "  batch 6000 loss: 8.415727120399476\n",
      "  batch 7000 loss: 8.40888502407074\n",
      "  batch 8000 loss: 8.411890878677369\n",
      "  batch 9000 loss: 8.407180623054504\n",
      "  batch 10000 loss: 8.417217804908752\n",
      "LOSS train8.417217804908752\n",
      "EPOCH 36:\n",
      "  batch 1000 loss: 8.404173090934753\n",
      "  batch 2000 loss: 8.400249857902526\n",
      "  batch 3000 loss: 8.400321957588195\n",
      "  batch 4000 loss: 8.406275119781494\n",
      "  batch 5000 loss: 8.402290095329285\n",
      "  batch 6000 loss: 8.396973045349121\n",
      "  batch 7000 loss: 8.395736168861388\n",
      "  batch 8000 loss: 8.397871960639954\n",
      "  batch 9000 loss: 8.393218626022339\n",
      "  batch 10000 loss: 8.391813910484315\n",
      "LOSS train8.391813910484315\n",
      "EPOCH 37:\n",
      "  batch 1000 loss: 8.391502511024475\n",
      "  batch 2000 loss: 8.382748918533325\n",
      "  batch 3000 loss: 8.385258407592774\n",
      "  batch 4000 loss: 8.38111350250244\n",
      "  batch 5000 loss: 8.385670929908752\n",
      "  batch 6000 loss: 8.37991331577301\n",
      "  batch 7000 loss: 8.381858318328858\n",
      "  batch 8000 loss: 8.385438069343566\n",
      "  batch 9000 loss: 8.380306834220887\n",
      "  batch 10000 loss: 8.380266231536865\n",
      "LOSS train8.380266231536865\n",
      "EPOCH 38:\n",
      "  batch 1000 loss: 8.375126663208007\n",
      "  batch 2000 loss: 8.369651051521302\n",
      "  batch 3000 loss: 8.377479027748107\n",
      "  batch 4000 loss: 8.375048774719238\n",
      "  batch 5000 loss: 8.37615132522583\n",
      "  batch 6000 loss: 8.37377777004242\n",
      "  batch 7000 loss: 8.373067634582519\n",
      "  batch 8000 loss: 8.371523844718933\n",
      "  batch 9000 loss: 8.369681925773621\n",
      "  batch 10000 loss: 8.36389340686798\n",
      "LOSS train8.36389340686798\n",
      "EPOCH 39:\n",
      "  batch 1000 loss: 8.363319607734681\n",
      "  batch 2000 loss: 8.365197340011596\n",
      "  batch 3000 loss: 8.363028662681579\n",
      "  batch 4000 loss: 8.370154534339905\n",
      "  batch 5000 loss: 8.36190805530548\n",
      "  batch 6000 loss: 8.363446226119995\n",
      "  batch 7000 loss: 8.353277963638305\n",
      "  batch 8000 loss: 8.359876663208007\n",
      "  batch 9000 loss: 8.358862941741943\n",
      "  batch 10000 loss: 8.35624799156189\n",
      "LOSS train8.35624799156189\n",
      "EPOCH 40:\n",
      "  batch 1000 loss: 8.353200947761536\n",
      "  batch 2000 loss: 8.36073429775238\n",
      "  batch 3000 loss: 8.350732789039611\n",
      "  batch 4000 loss: 8.351012561798095\n",
      "  batch 5000 loss: 8.354520186424255\n",
      "  batch 6000 loss: 8.351796278953552\n",
      "  batch 7000 loss: 8.352012508392335\n",
      "  batch 8000 loss: 8.348704133033753\n",
      "  batch 9000 loss: 8.353222382545471\n",
      "  batch 10000 loss: 8.351164151191712\n",
      "LOSS train8.351164151191712\n",
      "EPOCH 41:\n",
      "  batch 1000 loss: 8.350509523391723\n",
      "  batch 2000 loss: 8.34158542728424\n",
      "  batch 3000 loss: 8.337006218910217\n",
      "  batch 4000 loss: 8.349908025741577\n",
      "  batch 5000 loss: 8.345260869026184\n",
      "  batch 6000 loss: 8.346773529052735\n",
      "  batch 7000 loss: 8.340973348617554\n",
      "  batch 8000 loss: 8.340895560264588\n",
      "  batch 9000 loss: 8.339215606689454\n",
      "  batch 10000 loss: 8.337624393463134\n",
      "LOSS train8.337624393463134\n",
      "EPOCH 42:\n",
      "  batch 1000 loss: 8.337808215141296\n",
      "  batch 2000 loss: 8.34112765979767\n",
      "  batch 3000 loss: 8.343314113616943\n",
      "  batch 4000 loss: 8.336916773796082\n",
      "  batch 5000 loss: 8.332698725700379\n",
      "  batch 6000 loss: 8.337070105552673\n",
      "  batch 7000 loss: 8.334803946495056\n",
      "  batch 8000 loss: 8.331275191307068\n",
      "  batch 9000 loss: 8.329148114204406\n",
      "  batch 10000 loss: 8.331337144851684\n",
      "LOSS train8.331337144851684\n",
      "EPOCH 43:\n",
      "  batch 1000 loss: 8.330857626914979\n",
      "  batch 2000 loss: 8.324313711166381\n",
      "  batch 3000 loss: 8.329375991821289\n",
      "  batch 4000 loss: 8.333531755447387\n",
      "  batch 5000 loss: 8.32661709690094\n",
      "  batch 6000 loss: 8.329614245414733\n",
      "  batch 7000 loss: 8.328236409187317\n",
      "  batch 8000 loss: 8.326299286842346\n",
      "  batch 9000 loss: 8.331059002876282\n",
      "  batch 10000 loss: 8.325991814613342\n",
      "LOSS train8.325991814613342\n",
      "EPOCH 44:\n",
      "  batch 1000 loss: 8.326704048156738\n",
      "  batch 2000 loss: 8.318792761802673\n",
      "  batch 3000 loss: 8.32929676246643\n",
      "  batch 4000 loss: 8.319369603157044\n",
      "  batch 5000 loss: 8.320437071800232\n",
      "  batch 6000 loss: 8.319735646247864\n",
      "  batch 7000 loss: 8.319935096740723\n",
      "  batch 8000 loss: 8.313470362663269\n",
      "  batch 9000 loss: 8.320947005271911\n",
      "  batch 10000 loss: 8.319649753570557\n",
      "LOSS train8.319649753570557\n",
      "EPOCH 45:\n",
      "  batch 1000 loss: 8.319069745063782\n",
      "  batch 2000 loss: 8.31746360015869\n",
      "  batch 3000 loss: 8.315344612121581\n",
      "  batch 4000 loss: 8.314547315597535\n",
      "  batch 5000 loss: 8.316608308792114\n",
      "  batch 6000 loss: 8.315793986320495\n",
      "  batch 7000 loss: 8.315162096977234\n",
      "  batch 8000 loss: 8.314807208061218\n",
      "  batch 9000 loss: 8.315931659698487\n",
      "  batch 10000 loss: 8.30771970653534\n",
      "LOSS train8.30771970653534\n",
      "EPOCH 46:\n",
      "  batch 1000 loss: 8.306310116767884\n",
      "  batch 2000 loss: 8.308086213111878\n",
      "  batch 3000 loss: 8.312421178817749\n",
      "  batch 4000 loss: 8.308079233169556\n",
      "  batch 5000 loss: 8.306077907562257\n",
      "  batch 6000 loss: 8.311616870880126\n",
      "  batch 7000 loss: 8.305283394813538\n",
      "  batch 8000 loss: 8.309266031265258\n",
      "  batch 9000 loss: 8.304123846054077\n",
      "  batch 10000 loss: 8.308113879203797\n",
      "LOSS train8.308113879203797\n",
      "EPOCH 47:\n",
      "  batch 1000 loss: 8.299158123970031\n",
      "  batch 2000 loss: 8.308516010284423\n",
      "  batch 3000 loss: 8.308421748161315\n",
      "  batch 4000 loss: 8.305457571983338\n",
      "  batch 5000 loss: 8.301687847137451\n",
      "  batch 6000 loss: 8.306563735961914\n",
      "  batch 7000 loss: 8.304993618011475\n",
      "  batch 8000 loss: 8.300496554374694\n",
      "  batch 9000 loss: 8.303266414642334\n",
      "  batch 10000 loss: 8.308828364372253\n",
      "LOSS train8.308828364372253\n",
      "EPOCH 48:\n",
      "  batch 1000 loss: 8.304761655807495\n",
      "  batch 2000 loss: 8.303933436393738\n",
      "  batch 3000 loss: 8.300172792434692\n",
      "  batch 4000 loss: 8.30173788833618\n",
      "  batch 5000 loss: 8.296366577148438\n",
      "  batch 6000 loss: 8.304092471122742\n",
      "  batch 7000 loss: 8.299266171455383\n",
      "  batch 8000 loss: 8.295384508132935\n",
      "  batch 9000 loss: 8.29816462802887\n",
      "  batch 10000 loss: 8.29914005947113\n",
      "LOSS train8.29914005947113\n",
      "EPOCH 49:\n",
      "  batch 1000 loss: 8.299824061393737\n",
      "  batch 2000 loss: 8.301884172439575\n",
      "  batch 3000 loss: 8.295900434494019\n",
      "  batch 4000 loss: 8.295524517059325\n",
      "  batch 5000 loss: 8.293910581588746\n",
      "  batch 6000 loss: 8.29511344242096\n",
      "  batch 7000 loss: 8.291368639945984\n",
      "  batch 8000 loss: 8.299589173316956\n",
      "  batch 9000 loss: 8.292230136871337\n",
      "  batch 10000 loss: 8.290895396232605\n",
      "LOSS train8.290895396232605\n",
      "EPOCH 50:\n",
      "  batch 1000 loss: 8.292880613327027\n",
      "  batch 2000 loss: 8.289910181045531\n",
      "  batch 3000 loss: 8.293654994010925\n",
      "  batch 4000 loss: 8.291377255439759\n",
      "  batch 5000 loss: 8.289211320877076\n",
      "  batch 6000 loss: 8.294294688224792\n",
      "  batch 7000 loss: 8.296577103614807\n",
      "  batch 8000 loss: 8.287262168884277\n",
      "  batch 9000 loss: 8.284865006446838\n",
      "  batch 10000 loss: 8.287769196510315\n",
      "LOSS train8.287769196510315\n",
      "EPOCH 51:\n",
      "  batch 1000 loss: 8.289059296607972\n",
      "  batch 2000 loss: 8.287979685783386\n",
      "  batch 3000 loss: 8.290249562263488\n",
      "  batch 4000 loss: 8.285542279243469\n",
      "  batch 5000 loss: 8.287184646606445\n",
      "  batch 6000 loss: 8.284551727294922\n",
      "  batch 7000 loss: 8.284272944450379\n",
      "  batch 8000 loss: 8.284609063148498\n",
      "  batch 9000 loss: 8.284843153953553\n",
      "  batch 10000 loss: 8.288269486427307\n",
      "LOSS train8.288269486427307\n",
      "EPOCH 52:\n",
      "  batch 1000 loss: 8.285808987617493\n",
      "  batch 2000 loss: 8.284958243370056\n",
      "  batch 3000 loss: 8.282893397331238\n",
      "  batch 4000 loss: 8.28453619003296\n",
      "  batch 5000 loss: 8.288252818107605\n",
      "  batch 6000 loss: 8.283190587043762\n",
      "  batch 7000 loss: 8.282589105606078\n",
      "  batch 8000 loss: 8.28393832206726\n",
      "  batch 9000 loss: 8.280683537483215\n",
      "  batch 10000 loss: 8.281421074867248\n",
      "LOSS train8.281421074867248\n",
      "EPOCH 53:\n",
      "  batch 1000 loss: 8.27592310810089\n",
      "  batch 2000 loss: 8.284558458328247\n",
      "  batch 3000 loss: 8.281135657310486\n",
      "  batch 4000 loss: 8.281992621421814\n",
      "  batch 5000 loss: 8.278397309303283\n",
      "  batch 6000 loss: 8.279804181098937\n",
      "  batch 7000 loss: 8.280906116485596\n",
      "  batch 8000 loss: 8.278008237838746\n",
      "  batch 9000 loss: 8.276880153656005\n",
      "  batch 10000 loss: 8.276462456703186\n",
      "LOSS train8.276462456703186\n",
      "EPOCH 54:\n",
      "  batch 1000 loss: 8.279555735588074\n",
      "  batch 2000 loss: 8.27751638507843\n",
      "  batch 3000 loss: 8.273493479728698\n",
      "  batch 4000 loss: 8.27710691165924\n",
      "  batch 5000 loss: 8.27496428012848\n",
      "  batch 6000 loss: 8.2744547996521\n",
      "  batch 7000 loss: 8.276525756835937\n",
      "  batch 8000 loss: 8.27694947052002\n",
      "  batch 9000 loss: 8.272724524497987\n",
      "  batch 10000 loss: 8.274548593521118\n",
      "LOSS train8.274548593521118\n",
      "EPOCH 55:\n",
      "  batch 1000 loss: 8.273792077064515\n",
      "  batch 2000 loss: 8.274190020561218\n",
      "  batch 3000 loss: 8.272209192276001\n",
      "  batch 4000 loss: 8.275100063323974\n",
      "  batch 5000 loss: 8.274019233703614\n",
      "  batch 6000 loss: 8.272544010162353\n",
      "  batch 7000 loss: 8.274040761947631\n",
      "  batch 8000 loss: 8.272199346542358\n",
      "  batch 9000 loss: 8.274239259719849\n",
      "  batch 10000 loss: 8.272037783622741\n",
      "LOSS train8.272037783622741\n",
      "EPOCH 56:\n",
      "  batch 1000 loss: 8.269455045700074\n",
      "  batch 2000 loss: 8.27022601222992\n",
      "  batch 3000 loss: 8.267610510826112\n",
      "  batch 4000 loss: 8.272673139572143\n",
      "  batch 5000 loss: 8.268594114303589\n",
      "  batch 6000 loss: 8.267514554977417\n",
      "  batch 7000 loss: 8.26908845615387\n",
      "  batch 8000 loss: 8.26863185119629\n",
      "  batch 9000 loss: 8.27186228942871\n",
      "  batch 10000 loss: 8.268807638168335\n",
      "LOSS train8.268807638168335\n",
      "EPOCH 57:\n",
      "  batch 1000 loss: 8.268139354705811\n",
      "  batch 2000 loss: 8.26755885887146\n",
      "  batch 3000 loss: 8.266383415222167\n",
      "  batch 4000 loss: 8.267636400222779\n",
      "  batch 5000 loss: 8.267856313705444\n",
      "  batch 6000 loss: 8.26921984577179\n",
      "  batch 7000 loss: 8.269051826477051\n",
      "  batch 8000 loss: 8.26399508190155\n",
      "  batch 9000 loss: 8.26531435585022\n",
      "  batch 10000 loss: 8.262684328079224\n",
      "LOSS train8.262684328079224\n",
      "EPOCH 58:\n",
      "  batch 1000 loss: 8.26241073513031\n",
      "  batch 2000 loss: 8.263872515678406\n",
      "  batch 3000 loss: 8.268287447929382\n",
      "  batch 4000 loss: 8.263646579742431\n",
      "  batch 5000 loss: 8.26570641231537\n",
      "  batch 6000 loss: 8.26361922264099\n",
      "  batch 7000 loss: 8.26419029712677\n",
      "  batch 8000 loss: 8.26098844242096\n",
      "  batch 9000 loss: 8.26185775566101\n",
      "  batch 10000 loss: 8.26443604373932\n",
      "LOSS train8.26443604373932\n",
      "EPOCH 59:\n",
      "  batch 1000 loss: 8.260201085090637\n",
      "  batch 2000 loss: 8.263553499221802\n",
      "  batch 3000 loss: 8.260582604408263\n",
      "  batch 4000 loss: 8.262261863708495\n",
      "  batch 5000 loss: 8.259946542739868\n",
      "  batch 6000 loss: 8.26148833179474\n",
      "  batch 7000 loss: 8.261512014389037\n",
      "  batch 8000 loss: 8.262214146614074\n",
      "  batch 9000 loss: 8.259909882545472\n",
      "  batch 10000 loss: 8.257729978561402\n",
      "LOSS train8.257729978561402\n",
      "EPOCH 60:\n",
      "  batch 1000 loss: 8.260259316444397\n",
      "  batch 2000 loss: 8.261490962028503\n",
      "  batch 3000 loss: 8.25922464466095\n",
      "  batch 4000 loss: 8.258383428573609\n",
      "  batch 5000 loss: 8.259920925140381\n",
      "  batch 6000 loss: 8.256174847602844\n",
      "  batch 7000 loss: 8.259098719596862\n",
      "  batch 8000 loss: 8.259496939659119\n",
      "  batch 9000 loss: 8.261055881500244\n",
      "  batch 10000 loss: 8.258380792617798\n",
      "LOSS train8.258380792617798\n",
      "EPOCH 61:\n",
      "  batch 1000 loss: 8.259791041374207\n",
      "  batch 2000 loss: 8.259364876747131\n",
      "  batch 3000 loss: 8.259890663146972\n",
      "  batch 4000 loss: 8.25669619178772\n",
      "  batch 5000 loss: 8.255520084381104\n",
      "  batch 6000 loss: 8.25739223575592\n",
      "  batch 7000 loss: 8.255513600349426\n",
      "  batch 8000 loss: 8.255961177825927\n",
      "  batch 9000 loss: 8.255966983795165\n",
      "  batch 10000 loss: 8.257847180366516\n",
      "LOSS train8.257847180366516\n",
      "EPOCH 62:\n",
      "  batch 1000 loss: 8.257944369316101\n",
      "  batch 2000 loss: 8.256018699645995\n",
      "  batch 3000 loss: 8.25588368988037\n",
      "  batch 4000 loss: 8.256538474082946\n",
      "  batch 5000 loss: 8.255413270950317\n",
      "  batch 6000 loss: 8.255629651069642\n",
      "  batch 7000 loss: 8.255336297035218\n",
      "  batch 8000 loss: 8.253855289459228\n",
      "  batch 9000 loss: 8.259858602523805\n",
      "  batch 10000 loss: 8.253706906318664\n",
      "LOSS train8.253706906318664\n",
      "EPOCH 63:\n",
      "  batch 1000 loss: 8.252644430160522\n",
      "  batch 2000 loss: 8.255477416992187\n",
      "  batch 3000 loss: 8.256550267219543\n",
      "  batch 4000 loss: 8.255690727233887\n",
      "  batch 5000 loss: 8.253626697540284\n",
      "  batch 6000 loss: 8.253071914672852\n",
      "  batch 7000 loss: 8.255329451560975\n",
      "  batch 8000 loss: 8.251188256263733\n",
      "  batch 9000 loss: 8.254477067947388\n",
      "  batch 10000 loss: 8.25281583595276\n",
      "LOSS train8.25281583595276\n",
      "EPOCH 64:\n",
      "  batch 1000 loss: 8.250619988441468\n",
      "  batch 2000 loss: 8.2530532579422\n",
      "  batch 3000 loss: 8.251932043075561\n",
      "  batch 4000 loss: 8.251136721611022\n",
      "  batch 5000 loss: 8.250628967285156\n",
      "  batch 6000 loss: 8.250438076019288\n",
      "  batch 7000 loss: 8.250658773422241\n",
      "  batch 8000 loss: 8.250525153160096\n",
      "  batch 9000 loss: 8.251741921424866\n",
      "  batch 10000 loss: 8.250293153762817\n",
      "LOSS train8.250293153762817\n",
      "EPOCH 65:\n",
      "  batch 1000 loss: 8.248897317886353\n",
      "  batch 2000 loss: 8.251417109489442\n",
      "  batch 3000 loss: 8.250057148933411\n",
      "  batch 4000 loss: 8.24811678981781\n",
      "  batch 5000 loss: 8.249134020805359\n",
      "  batch 6000 loss: 8.2480737657547\n",
      "  batch 7000 loss: 8.249170528411865\n",
      "  batch 8000 loss: 8.2534419298172\n",
      "  batch 9000 loss: 8.248114633560181\n",
      "  batch 10000 loss: 8.247548331260681\n",
      "LOSS train8.247548331260681\n",
      "EPOCH 66:\n",
      "  batch 1000 loss: 8.249995350837708\n",
      "  batch 2000 loss: 8.247237317085267\n",
      "  batch 3000 loss: 8.246760692596435\n",
      "  batch 4000 loss: 8.247698360443115\n",
      "  batch 5000 loss: 8.24610140323639\n",
      "  batch 6000 loss: 8.247565495491028\n",
      "  batch 7000 loss: 8.248904685974122\n",
      "  batch 8000 loss: 8.247829344749452\n",
      "  batch 9000 loss: 8.249588594436645\n",
      "  batch 10000 loss: 8.24849997997284\n",
      "LOSS train8.24849997997284\n",
      "EPOCH 67:\n",
      "  batch 1000 loss: 8.247657204627991\n",
      "  batch 2000 loss: 8.24758489894867\n",
      "  batch 3000 loss: 8.249868915557862\n",
      "  batch 4000 loss: 8.249281212806702\n",
      "  batch 5000 loss: 8.246510328292846\n",
      "  batch 6000 loss: 8.245724913597106\n",
      "  batch 7000 loss: 8.244970196723939\n",
      "  batch 8000 loss: 8.246567556381226\n",
      "  batch 9000 loss: 8.24564359855652\n",
      "  batch 10000 loss: 8.246261067390442\n",
      "LOSS train8.246261067390442\n",
      "EPOCH 68:\n",
      "  batch 1000 loss: 8.246576484680176\n",
      "  batch 2000 loss: 8.245503617286682\n",
      "  batch 3000 loss: 8.243959421157838\n",
      "  batch 4000 loss: 8.245041389465332\n",
      "  batch 5000 loss: 8.24465010356903\n",
      "  batch 6000 loss: 8.24463382434845\n",
      "  batch 7000 loss: 8.244113178253174\n",
      "  batch 8000 loss: 8.245819638252259\n",
      "  batch 9000 loss: 8.246262929916382\n",
      "  batch 10000 loss: 8.246060117721557\n",
      "LOSS train8.246060117721557\n",
      "EPOCH 69:\n",
      "  batch 1000 loss: 8.245067267417907\n",
      "  batch 2000 loss: 8.245866864204407\n",
      "  batch 3000 loss: 8.243065043449402\n",
      "  batch 4000 loss: 8.244244092941285\n",
      "  batch 5000 loss: 8.246540882110596\n",
      "  batch 6000 loss: 8.243746855735779\n",
      "  batch 7000 loss: 8.243868434906005\n",
      "  batch 8000 loss: 8.243881118774414\n",
      "  batch 9000 loss: 8.24669822883606\n",
      "  batch 10000 loss: 8.242266709327698\n",
      "LOSS train8.242266709327698\n",
      "EPOCH 70:\n",
      "  batch 1000 loss: 8.243608686447143\n",
      "  batch 2000 loss: 8.242315441131591\n",
      "  batch 3000 loss: 8.243430585861207\n",
      "  batch 4000 loss: 8.241002683639527\n",
      "  batch 5000 loss: 8.243815947532655\n",
      "  batch 6000 loss: 8.24660419178009\n",
      "  batch 7000 loss: 8.243411241531373\n",
      "  batch 8000 loss: 8.246554242134094\n",
      "  batch 9000 loss: 8.241870460510254\n",
      "  batch 10000 loss: 8.242196825027467\n",
      "LOSS train8.242196825027467\n",
      "EPOCH 71:\n",
      "  batch 1000 loss: 8.242185542106629\n",
      "  batch 2000 loss: 8.243040343284607\n",
      "  batch 3000 loss: 8.243922512054443\n",
      "  batch 4000 loss: 8.241870748519897\n",
      "  batch 5000 loss: 8.241635818481445\n",
      "  batch 6000 loss: 8.243729044914245\n",
      "  batch 7000 loss: 8.241498083114625\n",
      "  batch 8000 loss: 8.24173934841156\n",
      "  batch 9000 loss: 8.240613086700439\n",
      "  batch 10000 loss: 8.241832299232483\n",
      "LOSS train8.241832299232483\n",
      "EPOCH 72:\n",
      "  batch 1000 loss: 8.241632697105407\n",
      "  batch 2000 loss: 8.24365732383728\n",
      "  batch 3000 loss: 8.242874750137329\n",
      "  batch 4000 loss: 8.243144033432007\n",
      "  batch 5000 loss: 8.244707674980164\n",
      "  batch 6000 loss: 8.241777574539185\n",
      "  batch 7000 loss: 8.242010190963745\n",
      "  batch 8000 loss: 8.241620681762695\n",
      "  batch 9000 loss: 8.240815547943114\n",
      "  batch 10000 loss: 8.241759693145752\n",
      "LOSS train8.241759693145752\n",
      "EPOCH 73:\n",
      "  batch 1000 loss: 8.241070754051208\n",
      "  batch 2000 loss: 8.242076479911804\n",
      "  batch 3000 loss: 8.241226378440857\n",
      "  batch 4000 loss: 8.240507545471191\n",
      "  batch 5000 loss: 8.24091669178009\n",
      "  batch 6000 loss: 8.241995204925537\n",
      "  batch 7000 loss: 8.242145550727844\n",
      "  batch 8000 loss: 8.239937929153442\n",
      "  batch 9000 loss: 8.239731039047241\n",
      "  batch 10000 loss: 8.2396803855896\n",
      "LOSS train8.2396803855896\n",
      "EPOCH 74:\n",
      "  batch 1000 loss: 8.240474681854248\n",
      "  batch 2000 loss: 8.239012711524964\n",
      "  batch 3000 loss: 8.239622878074647\n",
      "  batch 4000 loss: 8.239354441642762\n",
      "  batch 5000 loss: 8.239545243263244\n",
      "  batch 6000 loss: 8.241238073348999\n",
      "  batch 7000 loss: 8.241752568244934\n",
      "  batch 8000 loss: 8.239679228782654\n",
      "  batch 9000 loss: 8.239536805152893\n",
      "  batch 10000 loss: 8.239536186218261\n",
      "LOSS train8.239536186218261\n",
      "EPOCH 75:\n",
      "  batch 1000 loss: 8.239303318023682\n",
      "  batch 2000 loss: 8.240180152893066\n",
      "  batch 3000 loss: 8.238502245903016\n",
      "  batch 4000 loss: 8.239446788787841\n",
      "  batch 5000 loss: 8.239084659576417\n",
      "  batch 6000 loss: 8.240124780654908\n",
      "  batch 7000 loss: 8.238873847961425\n",
      "  batch 8000 loss: 8.238434653282166\n",
      "  batch 9000 loss: 8.239594507217408\n",
      "  batch 10000 loss: 8.237924457550049\n",
      "LOSS train8.237924457550049\n",
      "EPOCH 76:\n",
      "  batch 1000 loss: 8.237957374572755\n",
      "  batch 2000 loss: 8.239916223526\n",
      "  batch 3000 loss: 8.23979016494751\n",
      "  batch 4000 loss: 8.23968671131134\n",
      "  batch 5000 loss: 8.240123593330383\n",
      "  batch 6000 loss: 8.238414818763733\n",
      "  batch 7000 loss: 8.237160584449768\n",
      "  batch 8000 loss: 8.23880016040802\n",
      "  batch 9000 loss: 8.238067539215088\n",
      "  batch 10000 loss: 8.239955346107482\n",
      "LOSS train8.239955346107482\n",
      "EPOCH 77:\n",
      "  batch 1000 loss: 8.237197624206543\n",
      "  batch 2000 loss: 8.238411248207091\n",
      "  batch 3000 loss: 8.236516346931458\n",
      "  batch 4000 loss: 8.237118871688843\n",
      "  batch 5000 loss: 8.238395316123963\n",
      "  batch 6000 loss: 8.237871239662171\n",
      "  batch 7000 loss: 8.237019388198853\n",
      "  batch 8000 loss: 8.237337853431702\n",
      "  batch 9000 loss: 8.238613373756408\n",
      "  batch 10000 loss: 8.237700420379639\n",
      "LOSS train8.237700420379639\n",
      "EPOCH 78:\n",
      "  batch 1000 loss: 8.2374025182724\n",
      "  batch 2000 loss: 8.237684739112854\n",
      "  batch 3000 loss: 8.240055596351624\n",
      "  batch 4000 loss: 8.23644190979004\n",
      "  batch 5000 loss: 8.236404472351074\n",
      "  batch 6000 loss: 8.236800971984863\n",
      "  batch 7000 loss: 8.236946413040162\n",
      "  batch 8000 loss: 8.237214813232422\n",
      "  batch 9000 loss: 8.238164001464844\n",
      "  batch 10000 loss: 8.236286891937256\n",
      "LOSS train8.236286891937256\n",
      "EPOCH 79:\n",
      "  batch 1000 loss: 8.236258371353149\n",
      "  batch 2000 loss: 8.237211120605469\n",
      "  batch 3000 loss: 8.235824514389039\n",
      "  batch 4000 loss: 8.23611374092102\n",
      "  batch 5000 loss: 8.235170162200928\n",
      "  batch 6000 loss: 8.235685890197754\n",
      "  batch 7000 loss: 8.23583736038208\n",
      "  batch 8000 loss: 8.238630687713624\n",
      "  batch 9000 loss: 8.23571737766266\n",
      "  batch 10000 loss: 8.238718082427978\n",
      "LOSS train8.238718082427978\n",
      "EPOCH 80:\n",
      "  batch 1000 loss: 8.237005480766296\n",
      "  batch 2000 loss: 8.236503319740295\n",
      "  batch 3000 loss: 8.23711120223999\n",
      "  batch 4000 loss: 8.239653383255005\n",
      "  batch 5000 loss: 8.236067133903504\n",
      "  batch 6000 loss: 8.23563144493103\n",
      "  batch 7000 loss: 8.23469659805298\n",
      "  batch 8000 loss: 8.235663423538208\n",
      "  batch 9000 loss: 8.235315364837646\n",
      "  batch 10000 loss: 8.235517025947571\n",
      "LOSS train8.235517025947571\n",
      "EPOCH 81:\n",
      "  batch 1000 loss: 8.23578085899353\n",
      "  batch 2000 loss: 8.234563070297241\n",
      "  batch 3000 loss: 8.235232668876648\n",
      "  batch 4000 loss: 8.236400898933411\n",
      "  batch 5000 loss: 8.235426598548889\n",
      "  batch 6000 loss: 8.235322681427002\n",
      "  batch 7000 loss: 8.236640495300293\n",
      "  batch 8000 loss: 8.236419584274293\n",
      "  batch 9000 loss: 8.236390915870667\n",
      "  batch 10000 loss: 8.235753302574158\n",
      "LOSS train8.235753302574158\n",
      "EPOCH 82:\n",
      "  batch 1000 loss: 8.235094970703125\n",
      "  batch 2000 loss: 8.23433643436432\n",
      "  batch 3000 loss: 8.234200439453126\n",
      "  batch 4000 loss: 8.235493531227112\n",
      "  batch 5000 loss: 8.23619928264618\n",
      "  batch 6000 loss: 8.236894968986512\n",
      "  batch 7000 loss: 8.236652136802673\n",
      "  batch 8000 loss: 8.235069378852844\n",
      "  batch 9000 loss: 8.234495162010193\n",
      "  batch 10000 loss: 8.235516976356505\n",
      "LOSS train8.235516976356505\n",
      "EPOCH 83:\n",
      "  batch 1000 loss: 8.23582553768158\n",
      "  batch 2000 loss: 8.23428221988678\n",
      "  batch 3000 loss: 8.23442174720764\n",
      "  batch 4000 loss: 8.234601796150207\n",
      "  batch 5000 loss: 8.235312999725341\n",
      "  batch 6000 loss: 8.234114009857178\n",
      "  batch 7000 loss: 8.23657201385498\n",
      "  batch 8000 loss: 8.2346316280365\n",
      "  batch 9000 loss: 8.235202134132384\n",
      "  batch 10000 loss: 8.234809519767762\n",
      "LOSS train8.234809519767762\n",
      "EPOCH 84:\n",
      "  batch 1000 loss: 8.234836938858033\n",
      "  batch 2000 loss: 8.236076691627503\n",
      "  batch 3000 loss: 8.234146528244018\n",
      "  batch 4000 loss: 8.234104806900024\n",
      "  batch 5000 loss: 8.234604570388793\n",
      "  batch 6000 loss: 8.234120777130126\n",
      "  batch 7000 loss: 8.23510592842102\n",
      "  batch 8000 loss: 8.235080994606019\n",
      "  batch 9000 loss: 8.235193389892578\n",
      "  batch 10000 loss: 8.23352108001709\n",
      "LOSS train8.23352108001709\n",
      "EPOCH 85:\n",
      "  batch 1000 loss: 8.234862132072449\n",
      "  batch 2000 loss: 8.233167953491211\n",
      "  batch 3000 loss: 8.233385191917419\n",
      "  batch 4000 loss: 8.235085713386535\n",
      "  batch 5000 loss: 8.233723232269288\n",
      "  batch 6000 loss: 8.23408462047577\n",
      "  batch 7000 loss: 8.23493161869049\n",
      "  batch 8000 loss: 8.23480318069458\n",
      "  batch 9000 loss: 8.235720211029053\n",
      "  batch 10000 loss: 8.234645977020264\n",
      "LOSS train8.234645977020264\n",
      "EPOCH 86:\n",
      "  batch 1000 loss: 8.233240453720093\n",
      "  batch 2000 loss: 8.2331670255661\n",
      "  batch 3000 loss: 8.233786292076111\n",
      "  batch 4000 loss: 8.234160458564759\n",
      "  batch 5000 loss: 8.234173110961914\n",
      "  batch 6000 loss: 8.23267666721344\n",
      "  batch 7000 loss: 8.233546059608459\n",
      "  batch 8000 loss: 8.233871044158935\n",
      "  batch 9000 loss: 8.232838896751403\n",
      "  batch 10000 loss: 8.2331897315979\n",
      "LOSS train8.2331897315979\n",
      "EPOCH 87:\n",
      "  batch 1000 loss: 8.23288249206543\n",
      "  batch 2000 loss: 8.234812737464905\n",
      "  batch 3000 loss: 8.233221676826478\n",
      "  batch 4000 loss: 8.23248925113678\n",
      "  batch 5000 loss: 8.234308325767516\n",
      "  batch 6000 loss: 8.232610074996948\n",
      "  batch 7000 loss: 8.232223841667174\n",
      "  batch 8000 loss: 8.233640909194946\n",
      "  batch 9000 loss: 8.234138502120972\n",
      "  batch 10000 loss: 8.231928938865662\n",
      "LOSS train8.231928938865662\n",
      "EPOCH 88:\n",
      "  batch 1000 loss: 8.233456574440002\n",
      "  batch 2000 loss: 8.232325236320495\n",
      "  batch 3000 loss: 8.235768437385559\n",
      "  batch 4000 loss: 8.23240647125244\n",
      "  batch 5000 loss: 8.233046929359435\n",
      "  batch 6000 loss: 8.233311992645264\n",
      "  batch 7000 loss: 8.233987575531005\n",
      "  batch 8000 loss: 8.234609451293945\n",
      "  batch 9000 loss: 8.232691850662231\n",
      "  batch 10000 loss: 8.232335896492005\n",
      "LOSS train8.232335896492005\n",
      "EPOCH 89:\n",
      "  batch 1000 loss: 8.234452606201172\n",
      "  batch 2000 loss: 8.234071155548095\n",
      "  batch 3000 loss: 8.23258286857605\n",
      "  batch 4000 loss: 8.23359213256836\n",
      "  batch 5000 loss: 8.23355129814148\n",
      "  batch 6000 loss: 8.232325941085815\n",
      "  batch 7000 loss: 8.232416878700256\n",
      "  batch 8000 loss: 8.232360459327698\n",
      "  batch 9000 loss: 8.232402229309082\n",
      "  batch 10000 loss: 8.232858332633972\n",
      "LOSS train8.232858332633972\n",
      "EPOCH 90:\n",
      "  batch 1000 loss: 8.232309399604798\n",
      "  batch 2000 loss: 8.231629131317138\n",
      "  batch 3000 loss: 8.23411666584015\n",
      "  batch 4000 loss: 8.234076407432557\n",
      "  batch 5000 loss: 8.232579695701599\n",
      "  batch 6000 loss: 8.23134631919861\n",
      "  batch 7000 loss: 8.231976541519165\n",
      "  batch 8000 loss: 8.23239617538452\n",
      "  batch 9000 loss: 8.231748351097107\n",
      "  batch 10000 loss: 8.232250579833984\n",
      "LOSS train8.232250579833984\n",
      "EPOCH 91:\n",
      "  batch 1000 loss: 8.232285037040711\n",
      "  batch 2000 loss: 8.232464586257935\n",
      "  batch 3000 loss: 8.23286957359314\n",
      "  batch 4000 loss: 8.231689843177795\n",
      "  batch 5000 loss: 8.230914827346801\n",
      "  batch 6000 loss: 8.232617053031921\n",
      "  batch 7000 loss: 8.230935073852539\n",
      "  batch 8000 loss: 8.231658569335938\n",
      "  batch 9000 loss: 8.231194940567017\n",
      "  batch 10000 loss: 8.233691982269287\n",
      "LOSS train8.233691982269287\n",
      "EPOCH 92:\n",
      "  batch 1000 loss: 8.232260233879089\n",
      "  batch 2000 loss: 8.23071778011322\n",
      "  batch 3000 loss: 8.2305576171875\n",
      "  batch 4000 loss: 8.231775238990783\n",
      "  batch 5000 loss: 8.231291105270385\n",
      "  batch 6000 loss: 8.231106894493102\n",
      "  batch 7000 loss: 8.232952814102173\n",
      "  batch 8000 loss: 8.23320313167572\n",
      "  batch 9000 loss: 8.230455011367798\n",
      "  batch 10000 loss: 8.23218047428131\n",
      "LOSS train8.23218047428131\n",
      "EPOCH 93:\n",
      "  batch 1000 loss: 8.230905304908752\n",
      "  batch 2000 loss: 8.231264435768127\n",
      "  batch 3000 loss: 8.230924073219299\n",
      "  batch 4000 loss: 8.231015642166138\n",
      "  batch 5000 loss: 8.233358149528504\n",
      "  batch 6000 loss: 8.231200923919678\n",
      "  batch 7000 loss: 8.230187047958374\n",
      "  batch 8000 loss: 8.231298647880553\n",
      "  batch 9000 loss: 8.23171378993988\n",
      "  batch 10000 loss: 8.231262208938599\n",
      "LOSS train8.231262208938599\n",
      "EPOCH 94:\n",
      "  batch 1000 loss: 8.231151270866395\n",
      "  batch 2000 loss: 8.230652665138244\n",
      "  batch 3000 loss: 8.230940234184265\n",
      "  batch 4000 loss: 8.23046043586731\n",
      "  batch 5000 loss: 8.230221978187561\n",
      "  batch 6000 loss: 8.231128420829773\n",
      "  batch 7000 loss: 8.233044405937195\n",
      "  batch 8000 loss: 8.23103262424469\n",
      "  batch 9000 loss: 8.23072950553894\n",
      "  batch 10000 loss: 8.23215599155426\n",
      "LOSS train8.23215599155426\n",
      "EPOCH 95:\n",
      "  batch 1000 loss: 8.231142790794372\n",
      "  batch 2000 loss: 8.230487197875977\n",
      "  batch 3000 loss: 8.231354570388794\n",
      "  batch 4000 loss: 8.231597911834717\n",
      "  batch 5000 loss: 8.230314583778382\n",
      "  batch 6000 loss: 8.22999098110199\n",
      "  batch 7000 loss: 8.232949439048767\n",
      "  batch 8000 loss: 8.230541778564453\n",
      "  batch 9000 loss: 8.230558507919312\n",
      "  batch 10000 loss: 8.23181330394745\n",
      "LOSS train8.23181330394745\n",
      "EPOCH 96:\n",
      "  batch 1000 loss: 8.23005152606964\n",
      "  batch 2000 loss: 8.230258884429931\n",
      "  batch 3000 loss: 8.22977324295044\n",
      "  batch 4000 loss: 8.23058346748352\n",
      "  batch 5000 loss: 8.2297902135849\n",
      "  batch 6000 loss: 8.230163848876954\n",
      "  batch 7000 loss: 8.231746625900268\n",
      "  batch 8000 loss: 8.22948468875885\n",
      "  batch 9000 loss: 8.231816400527954\n",
      "  batch 10000 loss: 8.230185409545898\n",
      "LOSS train8.230185409545898\n",
      "EPOCH 97:\n",
      "  batch 1000 loss: 8.22959217262268\n",
      "  batch 2000 loss: 8.231126552581786\n",
      "  batch 3000 loss: 8.230374911308289\n",
      "  batch 4000 loss: 8.230065851211547\n",
      "  batch 5000 loss: 8.230175399780274\n",
      "  batch 6000 loss: 8.229818990707397\n",
      "  batch 7000 loss: 8.230694911956787\n",
      "  batch 8000 loss: 8.230146300315857\n",
      "  batch 9000 loss: 8.22935988330841\n",
      "  batch 10000 loss: 8.229942889213563\n",
      "LOSS train8.229942889213563\n",
      "EPOCH 98:\n",
      "  batch 1000 loss: 8.229702860832214\n",
      "  batch 2000 loss: 8.230698778152465\n",
      "  batch 3000 loss: 8.230146555900573\n",
      "  batch 4000 loss: 8.229050909042359\n",
      "  batch 5000 loss: 8.22960380268097\n",
      "  batch 6000 loss: 8.229860107421874\n",
      "  batch 7000 loss: 8.229087230682373\n",
      "  batch 8000 loss: 8.229322256088256\n",
      "  batch 9000 loss: 8.229573781967163\n",
      "  batch 10000 loss: 8.22902252483368\n",
      "LOSS train8.22902252483368\n",
      "EPOCH 99:\n",
      "  batch 1000 loss: 8.228866000175476\n",
      "  batch 2000 loss: 8.229943537712098\n",
      "  batch 3000 loss: 8.22859536933899\n",
      "  batch 4000 loss: 8.229434945106506\n",
      "  batch 5000 loss: 8.230795184135436\n",
      "  batch 6000 loss: 8.229176891326905\n",
      "  batch 7000 loss: 8.22930334186554\n",
      "  batch 8000 loss: 8.229094882965088\n",
      "  batch 9000 loss: 8.229051630020141\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [95], line 13\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[39m# Make sure gradient tracking is on, and do a pass over the data\u001b[39;00m\n\u001b[1;32m     12\u001b[0m model\u001b[39m.\u001b[39mtrain(\u001b[39mTrue\u001b[39;00m)\n\u001b[0;32m---> 13\u001b[0m avg_loss \u001b[39m=\u001b[39m train_one_epoch(epoch_number)\n\u001b[1;32m     15\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m'\u001b[39m\u001b[39mLOSS train\u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39mformat(avg_loss))\n\u001b[1;32m     17\u001b[0m epoch_number \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n",
      "Cell \u001b[0;32mIn [94], line 14\u001b[0m, in \u001b[0;36mtrain_one_epoch\u001b[0;34m(epoch_index)\u001b[0m\n\u001b[1;32m     11\u001b[0m optimizer\u001b[39m.\u001b[39mzero_grad()\n\u001b[1;32m     13\u001b[0m \u001b[39m# Make predictions for this batch\u001b[39;00m\n\u001b[0;32m---> 14\u001b[0m outputs \u001b[39m=\u001b[39m model(inputs)\n\u001b[1;32m     16\u001b[0m \u001b[39m# Compute the loss and its gradients\u001b[39;00m\n\u001b[1;32m     17\u001b[0m loss \u001b[39m=\u001b[39m loss_fn(outputs, labels)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/torch/nn/modules/module.py:1190\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1186\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1187\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1188\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1189\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1190\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1191\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "Cell \u001b[0;32mIn [90], line 20\u001b[0m, in \u001b[0;36mNetwork.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     18\u001b[0m x \u001b[39m=\u001b[39m F\u001b[39m.\u001b[39mrelu(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfc1(x))\n\u001b[1;32m     19\u001b[0m x \u001b[39m=\u001b[39m F\u001b[39m.\u001b[39mrelu(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfc2(x))\n\u001b[0;32m---> 20\u001b[0m x \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39;49msigmoid(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfc3(x))\n\u001b[1;32m     21\u001b[0m \u001b[39mreturn\u001b[39;00m x\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Initializing in a separate cell so we can easily add more epochs to the same run\n",
    "epoch_number = 0\n",
    "\n",
    "EPOCHS = 500\n",
    "\n",
    "best_vloss = 1_000_000.\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    print('EPOCH {}:'.format(epoch_number + 1))\n",
    "\n",
    "    # Make sure gradient tracking is on, and do a pass over the data\n",
    "    model.train(True)\n",
    "    avg_loss = train_one_epoch(epoch_number)\n",
    "\n",
    "    print('LOSS train{}'.format(avg_loss))\n",
    "\n",
    "    epoch_number += 1\n",
    "\n",
    "# from https://pytorch.org/tutorials/beginner/introyt/trainingyt.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), \"model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "saved_model = Network()\n",
    "saved_model.load_state_dict(torch.load(\"model\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(wordstr):\n",
    "    parts = wordstr.split(\" \")\n",
    "    categories = [\"zero\", \"one\", \"two\", \"three\",\n",
    "                    \"four\", \"five\", \"six\", \"seven\",\n",
    "                    \"eight\", \"nine\", \"ten\", \"eleven\", \"twelve\",\n",
    "                    \"thirteen\", \"fourteen\", \"fifteen\",\n",
    "                    \"sixteen\", \"seventeen\", \"eighteen\",\n",
    "                    \"nineteen\", \"twenty\", \"thirty\", \"forty\",\n",
    "                    \"fifty\", \"sixty\", \"seventy\", \"eighty\",\n",
    "                    \"ninety\", \"hundred\", \"thousand\"]\n",
    "    values = []\n",
    "    i = 0\n",
    "    for part in parts:\n",
    "        if part in categories and i < 6:\n",
    "            values += [categories.index(part) + i * 30]\n",
    "            i += 1\n",
    "    x = torch.zeros(1, 180)\n",
    "    ixs = torch.tensor(values)\n",
    "    input = x.index_add(1, ixs, torch.ones(1, len(values)))\n",
    "    return saved_model.forward(input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9158"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict(\"nine thousand one hundred and fifty\").argmax().tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1448\n"
     ]
    }
   ],
   "source": [
    "count = 0\n",
    "for i in range(10000):\n",
    "    if predict(convert_to_words(str(i))).argmax().tolist() == i:\n",
    "        count += 1\n",
    "print(count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "count = 0\n",
    "for i in range(10000):\n",
    "    if random.randint(0,9999) == i:\n",
    "        count += 1\n",
    "print(count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4330.8586\n"
     ]
    }
   ],
   "source": [
    "total = 0\n",
    "for i in range(10000):\n",
    "    total += abs(predict(convert_to_words(str(i))).argmax().tolist() - i)\n",
    "print(total / 10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3375.6591\n"
     ]
    }
   ],
   "source": [
    "total = 0\n",
    "for i in range(10000):\n",
    "    total += abs(random.randint(0,9999) - i)\n",
    "print(total / 10000)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.6 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
